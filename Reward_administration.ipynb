{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba62293c-c648-46b2-af54-5ac3caeacb5a",
   "metadata": {},
   "source": [
    "## Description\n",
    "This file is for our experiments of Reward Administration. \n",
    "\n",
    "Due to computer performance limitations, it is difficult to present the procedures of dozens of experiments within the same file. Therefore, we used this template file to modify the Rewards and record the results of our experiments. \n",
    "\n",
    "For these experiments, we experimented with $reward\\_of\\_draw \\in \\{-0.1, -0.2, -0.3, -0.4, -0.5,\\}$, and recorded the results of each experiment (See our report for more details).\n",
    "\n",
    "For better comparison, we fix hyperparameters $\\gamma = 0.95,\\beta = 0.05$.\n",
    "Also, for our experimental results to be reproducible, we set the random seed to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4ea996-ac63-4ce7-8863-e9b15aa6521b",
   "metadata": {},
   "source": [
    "## Information about this experiment instance\n",
    "\n",
    "rewards:\n",
    "\n",
    "$reward\\_of\\_checkmate = 1$,\n",
    "$reward\\_of\\_draw = -0.1$\n",
    "\n",
    "Final results:\n",
    "\n",
    "Average checkmates = ,\n",
    "Average rewards = ,\n",
    "Average steps = ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9979117-7c2f-41ed-9d5d-00f3cb4eeb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from degree_freedom_queen import *\n",
    "from degree_freedom_king1 import *\n",
    "from degree_freedom_king2 import *\n",
    "from generate_game import *\n",
    "from Chess_env_reward_change import *     ## use new environment\n",
    "\n",
    "size_board = 4\n",
    "\n",
    "# define EMA function\n",
    "\n",
    "def EMA(arr,period=500):    ## Function for computing exponential moving average\n",
    "    df = pd.DataFrame(arr)\n",
    "    return df.ewm(span=period,min_periods=period).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c50eb27a-dfae-4a0d-aee3-cba959633124",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SET DIFFERENT REWARDS\n",
    "\n",
    "## ------------------------------------------------------------\n",
    "r_checkmate = 1       ## reward of checkmate\n",
    "r_draw = -0.1         ## reward of draw\n",
    "## ------------------------------------------------------------\n",
    "\n",
    "## INITIALISE THE ENVIRONMENT\n",
    "np.random.seed(1)\n",
    "env=Chess_Env(size_board,r_checkmate,r_draw)    ## use constructor to set the rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b93255d-7cdc-4382-8ab5-b4c897eb93e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "S,X,allowed_a=env.Initialise_game()\n",
    "N_a=np.shape(allowed_a)[0]   # TOTAL NUMBER OF POSSIBLE ACTIONS (OUPUT SIZE)\n",
    "N_in=np.shape(X)[0]    ## INPUT SIZE\n",
    "\n",
    "# FIXED HYPERPARAMETERS \n",
    "epsilon_0 = 0.2     # STARTING VALUE OF EPSILON FOR THE EPSILON-GREEDY POLICY\n",
    "eta = 0.0035        # THE LEARNING RATE\n",
    "gamma = 0.95        # THE DISCOUNT FACTOR\n",
    "beta = 0.05         # THE PARAMETER SETS HOW QUICKLY THE VALUE OF EPSILON IS DECAYING\n",
    "\n",
    "N_episodes = 100000 # THE NUMBER OF GAMES TO BE PLAYED "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8924c319-0d70-40ea-91a8-6360e35e22cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our neural network, which can be a three-layers or a four-layers network, by changing the variable [N_h2].\n",
    "# Including initialize network, forward to compute q values and propagate to update neural network\n",
    "\n",
    "class Neural_network: \n",
    "    def __init__(self):\n",
    "        self.Xavier_init=True\n",
    "        self.N_h=200               ## NUMBER OF HIDDEN NODES\n",
    "        self.N_h2=100              ## NUMBER OF HIDDEN 2 NODES(set this to 0 if only need one hidden layer)\n",
    "        self.N_a=32                ## OUTPUT SIZE\n",
    "        self.N_in=58               ## INPUT SIZE (change this if change state representation)\n",
    "        self.eta=0.0035            ## THE LEARNING RATE\n",
    "\n",
    "    def parameter_init(self):      #INITIALIZE W PARAMETER\n",
    "        if self.Xavier_init:       #USE XAVIER INITIALIZATION METHOD\n",
    "            self.W1 = np.random.randn(self.N_h, self.N_in) * np.sqrt(1 / (self.N_in))\n",
    "            if self.N_h2>0:\n",
    "                self.W2 = np.random.randn(self.N_h2, self.N_h) * np.sqrt(1 / (self.N_h))\n",
    "                self.W3 = np.random.randn(self.N_a, self.N_h2) * np.sqrt(1 / (self.N_h2))\n",
    "            else:\n",
    "                self.W2 = np.random.randn(self.N_a, self.N_h) * np.sqrt(1 / (self.N_h))\n",
    "        else:\n",
    "            self.W1 = np.random.uniform(0,1,(self.N_h, self.N_in))\n",
    "            self.W2 = np.random.uniform(0,1,(self.N_a, self.N_h))\n",
    "\n",
    "            # The following normalises the random weights so that the sum of each row =1\n",
    "            self.W1 = np.divide(self.W1,np.matlib.repmat(np.sum(self.W1,1)[:,None],1,self.N_in))\n",
    "            self.W2 = np.divide(self.W2,np.matlib.repmat(np.sum(self.W2,1)[:,None],1,self.N_h))\n",
    "\n",
    "            if self.N_h2>0:\n",
    "                self.W3=np.random.uniform(0,1,(self.N_a,self.N_h2))\n",
    "                self.W3=np.divide(self.W3,np.matlib.repmat(np.sum(self.W3,1)[:,None],1,self.N_h2))\n",
    "\n",
    "                self.W2=np.random.uniform(0,1,(self.N_h2,self.N_h))\n",
    "                self.W2=np.divide(self.W2,np.matlib.repmat(np.sum(self.W2,1)[:,None],1,self.N_h))\n",
    "                \n",
    "    def biases_init(self):    # INITIALIZE BIAS PARAMETER\n",
    "        self.bias_W1 = np.zeros((self.N_h,))\n",
    "        self.bias_W2 = np.zeros((self.N_a,))\n",
    "\n",
    "        if self.N_h2>0:    \n",
    "            self.bias_W3=np.zeros((self.N_a,))\n",
    "            self.bias_W2=np.zeros((self.N_h2,))\n",
    "\n",
    "    def predict(self,X):     # COMPUTE Q VALUES\n",
    "\n",
    "            # Neural activation: input layer -> hidden layer\n",
    "            self.h1 = np.dot(self.W1,X)+self.bias_W1\n",
    "\n",
    "            # Apply the leaky relu function\n",
    "            self.x1=np.where(self.h1>0,self.h1,0.01*self.h1)\n",
    "\n",
    "            # Neural activation: hidden layer -> output layer\n",
    "            self.h2 = np.dot(self.W2,self.x1)+self.bias_W2\n",
    "\n",
    "            # Apply the leaky relu function\n",
    "            self.x2=np.where(self.h2>0,self.h2,0.01*self.h2)\n",
    "\n",
    "            if self.N_h2 > 0:\n",
    "                # Neural activation: hidden layer 1 -> hidden layer 2\n",
    "                self.h3 = np.dot(self.W3,self.x2)+self.bias_W3\n",
    "                self.q_values = np.where(self.h3>0,self.h3,0.01*self.h3)\n",
    "\n",
    "            else:\n",
    "                self.q_values=self.x2\n",
    "\n",
    "            return self.q_values\n",
    "            \n",
    "    def update(self,delta,a,X):    # BACKWARD PROPAGATION\n",
    "        # Initialise the gradients for each batch\n",
    "        self.dW1 = np.zeros(self.W1.shape)\n",
    "        self.dW2 = np.zeros(self.W2.shape)\n",
    "\n",
    "        self.dbias_W1 = np.zeros(self.bias_W1.shape)\n",
    "        self.dbias_W2 = np.zeros(self.bias_W2.shape)\n",
    "\n",
    "        if self.N_h2 > 0:\n",
    "            self.dW3 = np.zeros(self.W3.shape)\n",
    "            self.dbias_W3 = np.zeros(self.bias_W3.shape)\n",
    "            \n",
    "        # Backpropagation\n",
    "        if self.N_h2 > 0:\n",
    "            self.delta3 = np.zeros(self.N_a)\n",
    "            self.delta3[a] = delta # only the action nueron\n",
    "            \n",
    "            # Backpropagation: output layer -> hidden layer 2\n",
    "            self.dW3 = np.outer(self.delta3,self.x2)\n",
    "            self.dbias_W3 = self.delta3\n",
    "\n",
    "            # Backpropagation: hidden layer 2 -> hidden layer 1\n",
    "            self.delta2 = np.dot(self.W3.T, self.delta3)\n",
    "\n",
    "        else:   \n",
    "            self.delta2 = np.zeros(self.N_a)\n",
    "            self.delta2[a] = delta\n",
    "        \n",
    "        # Backpropagation: output layer -> hidden layer 1\n",
    "        self.dW2 = np.outer(self.delta2, self.x1)\n",
    "        self.dbias_W2 = self.delta2\n",
    "\n",
    "        # Backpropagation: hidden layer -> input layer\n",
    "        self.delta1 = np.dot(self.W2.T, self.delta2)\n",
    "        self.dW1 = np.outer(self.delta1,X)\n",
    "        self.dbias_W1 = self.delta1\n",
    "\n",
    "        # update the weights using gradients\n",
    "        self.W2 += self.eta*self.dW2\n",
    "        self.W1 += self.eta*self.dW1\n",
    "\n",
    "        self.bias_W1 += self.eta*self.dbias_W1\n",
    "        self.bias_W2 += self.eta*self.dbias_W2\n",
    "\n",
    "        if self.N_h2 > 0:\n",
    "            self.W3 += self.eta*self.dW3\n",
    "            self.bias_W3 += self.eta*self.dbias_W3\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "746004c1-c888-4f68-b019-72cf66e92cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function training network using SARSA rule\n",
    "def train_model(X,action,reward,next_state,next_action,done): \n",
    "    q_values = model.predict(X)\n",
    "    if done:\n",
    "        delta=reward-q_values[action]\n",
    "    else:\n",
    "        delta=reward+gamma*model.predict(next_state)[next_action]-q_values[action] # sarsa update rule\n",
    "    # Update the weights\n",
    "    model.update(delta,action,X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5089eb2c-f533-443c-957f-efda4334f8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function choosing action according to epsilon greedy policy\n",
    "def get_action(X,a):      # X: current state; a: possible actions in the current state\n",
    "    if np.random.rand() <= epsilon_f:   # choosing action randomly\n",
    "        a_agent=np.random.permutation(a)[0]\n",
    "    else:\n",
    "        q_values = model.predict(X)     # Predict Q value based on given state\n",
    "        Qvalues = np.copy(q_values[a])  # only the possible actions in the current state\n",
    "        a_max = np.argmax(Qvalues)      # choose the action with max Q value\n",
    "        a_agent = np.copy(a[a_max])     # return the index of action\n",
    "    return a_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "368b2ec9-1698-40f3-9481-a3235da43453",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodes: 0\n",
      "Episodes: 1000\n",
      "Episodes: 2000\n",
      "Episodes: 3000\n",
      "Episodes: 4000\n",
      "Episodes: 5000\n",
      "Episodes: 6000\n",
      "Episodes: 7000\n",
      "Episodes: 8000\n",
      "Episodes: 9000\n",
      "Episodes: 10000\n",
      "Episodes: 11000\n",
      "Episodes: 12000\n",
      "Episodes: 13000\n",
      "Episodes: 14000\n",
      "Episodes: 15000\n",
      "Episodes: 16000\n",
      "Episodes: 17000\n",
      "Episodes: 18000\n",
      "Episodes: 19000\n",
      "Episodes: 20000\n",
      "Episodes: 21000\n",
      "Episodes: 22000\n",
      "Episodes: 23000\n",
      "Episodes: 24000\n",
      "Episodes: 25000\n",
      "Episodes: 26000\n",
      "Episodes: 27000\n",
      "Episodes: 28000\n",
      "Episodes: 29000\n",
      "Episodes: 30000\n",
      "Episodes: 31000\n",
      "Episodes: 32000\n",
      "Episodes: 33000\n",
      "Episodes: 34000\n",
      "Episodes: 35000\n",
      "Episodes: 36000\n",
      "Episodes: 37000\n",
      "Episodes: 38000\n",
      "Episodes: 39000\n",
      "Episodes: 40000\n",
      "Episodes: 41000\n",
      "Episodes: 42000\n",
      "Episodes: 43000\n",
      "Episodes: 44000\n",
      "Episodes: 45000\n",
      "Episodes: 46000\n",
      "Episodes: 47000\n",
      "Episodes: 48000\n",
      "Episodes: 49000\n",
      "Episodes: 50000\n",
      "Episodes: 51000\n",
      "Episodes: 52000\n",
      "Episodes: 53000\n",
      "Episodes: 54000\n",
      "Episodes: 55000\n",
      "Episodes: 56000\n",
      "Episodes: 57000\n",
      "Episodes: 58000\n",
      "Episodes: 59000\n",
      "Episodes: 60000\n",
      "Episodes: 61000\n",
      "Episodes: 62000\n",
      "Episodes: 63000\n",
      "Episodes: 64000\n",
      "Episodes: 65000\n",
      "Episodes: 66000\n",
      "Episodes: 67000\n",
      "Episodes: 68000\n",
      "Episodes: 69000\n",
      "Episodes: 70000\n",
      "Episodes: 71000\n",
      "Episodes: 72000\n",
      "Episodes: 73000\n",
      "Episodes: 74000\n",
      "Episodes: 75000\n",
      "Episodes: 76000\n",
      "Episodes: 77000\n",
      "Episodes: 78000\n",
      "Episodes: 79000\n",
      "Episodes: 80000\n",
      "Episodes: 81000\n",
      "Episodes: 82000\n",
      "Episodes: 83000\n",
      "Episodes: 84000\n",
      "Episodes: 85000\n",
      "Episodes: 86000\n",
      "Episodes: 87000\n",
      "Episodes: 88000\n",
      "Episodes: 89000\n",
      "Episodes: 90000\n",
      "Episodes: 91000\n",
      "Episodes: 92000\n",
      "Episodes: 93000\n",
      "Episodes: 94000\n",
      "Episodes: 95000\n",
      "Episodes: 96000\n",
      "Episodes: 97000\n",
      "Episodes: 98000\n",
      "Episodes: 99000\n",
      "Deep_sarsa_Agent, Number of checkmates:  0.95609 Average reward: 0.951699 Number of steps:  5.26811\n"
     ]
    }
   ],
   "source": [
    "# TRAINING LOOP\n",
    "\n",
    "# INIALIZE THE NUERAL NETWORK WE USE\n",
    "model=Neural_network()\n",
    "model.parameter_init()\n",
    "model.biases_init()\n",
    "\n",
    "# SAVING VARIABLES\n",
    "R_save = np.zeros([N_episodes, 1])           ## STORE THE REWARD PER GAME\n",
    "N_moves_save = np.zeros([N_episodes, 1])     ## STOR THE NUMBER OF MOVES PER GAME\n",
    "N_checkmate_save = np.zeros([N_episodes, 1]) ## STOR THE checkmate case for counting the rate of it\n",
    "\n",
    "\n",
    "for n in range(N_episodes):\n",
    "    \n",
    "    if n % 1000 == 0:\n",
    "        print(\"Episodes:\", n)                ## print it every 1000 episodes\n",
    "\n",
    "    epsilon_f = epsilon_0 / (1 + beta * n)   ## DECAYING EPSILON\n",
    "    Done=0                                   ## SET DONE TO ZERO (BEGINNING OF THE EPISODE)\n",
    "    i = 1                                    ## COUNTER FOR NUMBER OF ACTIONS\n",
    "    \n",
    "    S,X,allowed_a=env.Initialise_game()      ## INITIALISE GAME (S: CHESSBOARD; X:STATE; ALLOWED_A: POSSIBLE ACTIONS)\n",
    "    \n",
    "    a,_=np.where(allowed_a==1)               ## THE POSSIBLE ACTIONS IN THE CURRENT STATE\n",
    "    a_agent = get_action(X,a)                ## CHOOSE A ACTION\n",
    "    \n",
    "    while Done==0:                           ## START THE EPISODE\n",
    "        \n",
    "\n",
    "        S_next,X_next,allowed_a_next,R,Done=env.OneStep(a_agent)  ## IMPLEMENT THE ACTION\n",
    "\n",
    "        ## THE EPISODE HAS ENDED, UPDATE\n",
    "        if Done==1:\n",
    "            \n",
    "            R_save[n]=np.copy(R)             ## STORE THE REWARD\n",
    "            N_moves_save[n]=np.copy(i)       ## STORE THE NUMBER OF MOVES\n",
    "            \n",
    "            N_checkmate_save[n] = np.copy(env.is_checkmate)  ## STORE THE CHECKMATE or DRAW CASE\n",
    "            \n",
    "            next_a_agent=0                   ## SET THIS TO 0 AS NO NEXT ACTION NEEDED\n",
    "            train_model(X,a_agent,R,X_next,next_a_agent,Done)  ## TRAIN MODEL AND UPDATE PARAMETER\n",
    "            break    \n",
    "        \n",
    "        # IF THE EPISODE IS NOT OVER...\n",
    "        else:\n",
    "            a_next,_=np.where(allowed_a_next==1)              ## THE POSSIBLE NEXT ACTIONS IN THE NEXT STATE\n",
    "            next_a_agent = get_action(X_next,a_next)          ## CHOOSE A ACTION FOR NEXT STATE\n",
    "            train_model(X,a_agent,R,X_next,next_a_agent,Done) ## TRAIN MODEL AND UPDATE PARAMETER\n",
    "            \n",
    "        # NEXT STATE AND CO. BECOME ACTUAL STATE...     \n",
    "        S=np.copy(S_next)                                     \n",
    "        X=np.copy(X_next)\n",
    "        allowed_a=np.copy(allowed_a_next)\n",
    "        a_agent=next_a_agent\n",
    "        \n",
    "        i += 1  # UPDATE COUNTER FOR NUMBER OF ACTIONS\n",
    "\n",
    "print('Deep_sarsa_Agent, Number of checkmates: ',np.mean(N_checkmate_save),\n",
    "      'Average reward:',np.mean(R_save),\n",
    "      'Number of steps: ',np.mean(N_moves_save))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2d9156d-13b8-4f4f-95d6-b9fdec09d817",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_filename = f'result_hyperparameters_{r_draw}.xlsx' # Create a corresponding file name\n",
    "\n",
    "g=pd.DataFrame()\n",
    "g['R_save']=R_save.reshape(N_episodes)\n",
    "g['N_moves_save']=N_moves_save.reshape(N_episodes)\n",
    "g['N_checkmate_save']=N_checkmate_save.reshape(N_episodes)\n",
    "\n",
    "g.to_excel(result_filename, encoding='utf-8',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2dd422f-a244-4e22-ae9c-ca0f19210ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "re=pd.read_excel(result_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fe98a9-46e2-4896-831a-f7a5e04ccb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(100000), EMA(re['N_checkmate_save']),c='blue',alpha=0.5,label='SARSA_reward_change')\n",
    "plt.legend()\n",
    "plt.title('The checkmate per game vs training time(EMA)')\n",
    "plt.xlabel('training time')\n",
    "plt.ylabel('The checkmate per game')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d223c280-8a8a-4831-bc7c-96a6078dcd30",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(N_episodes), EMA(re['R_save']),c='blue',alpha=0.5,label='SARSA_reward_change')\n",
    "plt.legend()\n",
    "plt.title('The reward per game vs training time(EMA)')\n",
    "plt.xlabel('training time')\n",
    "plt.ylabel('The reward per game')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c834bac-a06e-48ad-ba70-c4efe6699edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(N_episodes), EMA(re['N_moves_save']),c='b',alpha=0.5,label='SARSA_reward_change')\n",
    "plt.legend()\n",
    "plt.title('The number of moves per game vs training time(EMA)')\n",
    "plt.xlabel('training time')\n",
    "plt.ylabel('The number of moves per game')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65967f7c-0600-49d4-afba-6782a1fe778c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8e02c7-786f-4f18-bd52-9e063353f086",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
