{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "This file is for our experiments of Hyperparameter Modification. \n",
    "\n",
    "Due to computer performance limitations, it is difficult to present the procedures of dozens of experiments within the same file. Therefore, we used this template file to modify the parameters and record the results of our experiments. \n",
    "\n",
    "For these experiments, we experimented with each combination of $\\gamma \\in \\{0.85, 0.90, 0.95, 0.99\\}$ and $\\beta \\in \\{0.00005, 0.0005, 0.005, 0.05, 0.5\\}$, and recorded the results of each experiment (See our report for more details).\n",
    "\n",
    "Also, for our experimental results to be reproducible, we set the random seed to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information about this experiment instance\n",
    "\n",
    "Hyperparameters:\n",
    "$\\gamma = 0.95$,\n",
    "$\\beta = 0.05$\n",
    "\n",
    "Final results:\n",
    "Average rewards = , \n",
    "Average steps = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 520,
     "status": "ok",
     "timestamp": 1646428231662,
     "user": {
      "displayName": "段辉然",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04583225436557064813"
     },
     "user_tz": -60
    },
    "id": "sYAzBUaSp-BE"
   },
   "outputs": [],
   "source": [
    "# Import\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from degree_freedom_queen import *\n",
    "from degree_freedom_king1 import *\n",
    "from degree_freedom_king2 import *\n",
    "from generate_game import *\n",
    "from Chess_env import *\n",
    "\n",
    "size_board = 4\n",
    "\n",
    "# define EMA function\n",
    "\n",
    "def EMA(arr,period=500):    ## Function for computing exponential moving average\n",
    "    df = pd.DataFrame(arr)\n",
    "    return df.ewm(span=period,min_periods=period).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6bjXLuJAtoz6"
   },
   "source": [
    "## Define parameters and nueral network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1646428231662,
     "user": {
      "displayName": "段辉然",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04583225436557064813"
     },
     "user_tz": -60
    },
    "id": "6hIe7Z1Hug03"
   },
   "outputs": [],
   "source": [
    "## INITIALISE THE ENVIRONMENT\n",
    "np.random.seed(1)\n",
    "env=Chess_Env(size_board)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1646428232188,
     "user": {
      "displayName": "段辉然",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04583225436557064813"
     },
     "user_tz": -60
    },
    "id": "PWRsBvsCuhBJ"
   },
   "outputs": [],
   "source": [
    "S,X,allowed_a=env.Initialise_game()\n",
    "N_a=np.shape(allowed_a)[0]   # TOTAL NUMBER OF POSSIBLE ACTIONS (OUPUT SIZE)\n",
    "N_in=np.shape(X)[0]    ## INPUT SIZE\n",
    "\n",
    "# HYPERPARAMETERS WHICH CAN BE CHANGED ----------------------------------------------------\n",
    "gamma = 0.95        # THE DISCOUNT FACTOR\n",
    "beta = 0.05         # THE PARAMETER SETS HOW QUICKLY THE VALUE OF EPSILON IS DECAYING\n",
    "# -----------------------------------------------------------------------------------------\n",
    "\n",
    "# FIXED HYPERPARAMETERS \n",
    "epsilon_0 = 0.2     # STARTING VALUE OF EPSILON FOR THE EPSILON-GREEDY POLICY\n",
    "eta = 0.0035        # THE LEARNING RATE\n",
    "\n",
    "N_episodes = 100000 # THE NUMBER OF GAMES TO BE PLAYED \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1646428232188,
     "user": {
      "displayName": "段辉然",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04583225436557064813"
     },
     "user_tz": -60
    },
    "id": "7PoUcOIBuhDL"
   },
   "outputs": [],
   "source": [
    "# Define our neural network, which can be a three-layers or a four-layers network, by changing the variable [N_h2].\n",
    "# Including initialize network, forward to compute q values and propagate to update neural network\n",
    "\n",
    "class Neural_network: \n",
    "    def __init__(self):\n",
    "        self.Xavier_init=True\n",
    "        self.N_h=200               ## NUMBER OF HIDDEN NODES\n",
    "        self.N_h2=100              ## NUMBER OF HIDDEN 2 NODES(set this to 0 if only need one hidden layer)\n",
    "        self.N_a=32                ## OUTPUT SIZE\n",
    "        self.N_in=58               ## INPUT SIZE (change this if change state representation)\n",
    "        self.eta=0.0035            ## THE LEARNING RATE\n",
    "\n",
    "    def parameter_init(self):      #INITIALIZE W PARAMETER\n",
    "        if self.Xavier_init:       #USE XAVIER INITIALIZATION METHOD\n",
    "            self.W1 = np.random.randn(self.N_h, self.N_in) * np.sqrt(1 / (self.N_in))\n",
    "            if self.N_h2>0:\n",
    "                self.W2 = np.random.randn(self.N_h2, self.N_h) * np.sqrt(1 / (self.N_h))\n",
    "                self.W3 = np.random.randn(self.N_a, self.N_h2) * np.sqrt(1 / (self.N_h2))\n",
    "            else:\n",
    "                self.W2 = np.random.randn(self.N_a, self.N_h) * np.sqrt(1 / (self.N_h))\n",
    "        else:\n",
    "            self.W1 = np.random.uniform(0,1,(self.N_h, self.N_in))\n",
    "            self.W2 = np.random.uniform(0,1,(self.N_a, self.N_h))\n",
    "\n",
    "            # The following normalises the random weights so that the sum of each row =1\n",
    "            self.W1 = np.divide(self.W1,np.matlib.repmat(np.sum(self.W1,1)[:,None],1,self.N_in))\n",
    "            self.W2 = np.divide(self.W2,np.matlib.repmat(np.sum(self.W2,1)[:,None],1,self.N_h))\n",
    "\n",
    "            if self.N_h2>0:\n",
    "                self.W3=np.random.uniform(0,1,(self.N_a,self.N_h2))\n",
    "                self.W3=np.divide(self.W3,np.matlib.repmat(np.sum(self.W3,1)[:,None],1,self.N_h2))\n",
    "\n",
    "                self.W2=np.random.uniform(0,1,(self.N_h2,self.N_h))\n",
    "                self.W2=np.divide(self.W2,np.matlib.repmat(np.sum(self.W2,1)[:,None],1,self.N_h))\n",
    "                \n",
    "    def biases_init(self):    # INITIALIZE BIAS PARAMETER\n",
    "        self.bias_W1 = np.zeros((self.N_h,))\n",
    "        self.bias_W2 = np.zeros((self.N_a,))\n",
    "\n",
    "        if self.N_h2>0:    \n",
    "            self.bias_W3=np.zeros((self.N_a,))\n",
    "            self.bias_W2=np.zeros((self.N_h2,))\n",
    "\n",
    "    def predict(self,X):     # COMPUTE Q VALUES\n",
    "\n",
    "            # Neural activation: input layer -> hidden layer\n",
    "            self.h1 = np.dot(self.W1,X)+self.bias_W1\n",
    "\n",
    "            # Apply the leaky relu function\n",
    "            self.x1=np.where(self.h1>0,self.h1,0.01*self.h1)\n",
    "\n",
    "            # Neural activation: hidden layer -> output layer\n",
    "            self.h2 = np.dot(self.W2,self.x1)+self.bias_W2\n",
    "\n",
    "            # Apply the leaky relu function\n",
    "            self.x2=np.where(self.h2>0,self.h2,0.01*self.h2)\n",
    "\n",
    "            if self.N_h2 > 0:\n",
    "                # Neural activation: hidden layer 1 -> hidden layer 2\n",
    "                self.h3 = np.dot(self.W3,self.x2)+self.bias_W3\n",
    "                self.q_values = np.where(self.h3>0,self.h3,0.01*self.h3)\n",
    "\n",
    "            else:\n",
    "                self.q_values=self.x2\n",
    "\n",
    "            return self.q_values\n",
    "            \n",
    "    def update(self,delta,a,X):    # BACKWARD PROPAGATION\n",
    "        # Initialise the gradients for each batch\n",
    "        self.dW1 = np.zeros(self.W1.shape)\n",
    "        self.dW2 = np.zeros(self.W2.shape)\n",
    "\n",
    "        self.dbias_W1 = np.zeros(self.bias_W1.shape)\n",
    "        self.dbias_W2 = np.zeros(self.bias_W2.shape)\n",
    "\n",
    "        if self.N_h2 > 0:\n",
    "            self.dW3 = np.zeros(self.W3.shape)\n",
    "            self.dbias_W3 = np.zeros(self.bias_W3.shape)\n",
    "            \n",
    "        # Backpropagation\n",
    "        if self.N_h2 > 0:\n",
    "            self.delta3 = np.zeros(self.N_a)\n",
    "            self.delta3[a] = delta # only the action nueron\n",
    "            \n",
    "            # Backpropagation: output layer -> hidden layer 2\n",
    "            self.dW3 = np.outer(self.delta3,self.x2)\n",
    "            self.dbias_W3 = self.delta3\n",
    "\n",
    "            # Backpropagation: hidden layer 2 -> hidden layer 1\n",
    "            self.delta2 = np.dot(self.W3.T, self.delta3)\n",
    "\n",
    "        else:   \n",
    "            self.delta2 = np.zeros(self.N_a)\n",
    "            self.delta2[a] = delta\n",
    "        \n",
    "        # Backpropagation: output layer -> hidden layer 1\n",
    "        self.dW2 = np.outer(self.delta2, self.x1)\n",
    "        self.dbias_W2 = self.delta2\n",
    "\n",
    "        # Backpropagation: hidden layer -> input layer\n",
    "        self.delta1 = np.dot(self.W2.T, self.delta2)\n",
    "        self.dW1 = np.outer(self.delta1,X)\n",
    "        self.dbias_W1 = self.delta1\n",
    "\n",
    "        # update the weights using gradients\n",
    "        self.W2 += self.eta*self.dW2\n",
    "        self.W1 += self.eta*self.dW1\n",
    "\n",
    "        self.bias_W1 += self.eta*self.dbias_W1\n",
    "        self.bias_W2 += self.eta*self.dbias_W2\n",
    "\n",
    "        if self.N_h2 > 0:\n",
    "            self.W3 += self.eta*self.dW3\n",
    "            self.bias_W3 += self.eta*self.dbias_W3\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mhIJv-ZdvS3_"
   },
   "source": [
    "## SARSA Hyperparameter change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1646428232188,
     "user": {
      "displayName": "段辉然",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04583225436557064813"
     },
     "user_tz": -60
    },
    "id": "04o84nVTvRz2"
   },
   "outputs": [],
   "source": [
    "# Function training network using SARSA rule\n",
    "def train_model(X,action,reward,next_state,next_action,done): \n",
    "    q_values = model.predict(X)\n",
    "    if done:\n",
    "        delta=reward-q_values[action]\n",
    "    else:\n",
    "        delta=reward+gamma*model.predict(next_state)[next_action]-q_values[action] # sarsa update rule\n",
    "    # Update the weights\n",
    "    model.update(delta,action,X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1646428232189,
     "user": {
      "displayName": "段辉然",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04583225436557064813"
     },
     "user_tz": -60
    },
    "id": "EwTLHmgXvRxA"
   },
   "outputs": [],
   "source": [
    "# Function choosing action according to epsilon greedy policy\n",
    "def get_action(X,a):      # X: current state; a: possible actions in the current state\n",
    "    if np.random.rand() <= epsilon_f:   # choosing action randomly\n",
    "        a_agent=np.random.permutation(a)[0]\n",
    "    else:\n",
    "        q_values = model.predict(X)     # Predict Q value based on given state\n",
    "        Qvalues = np.copy(q_values[a])  # only the possible actions in the current state\n",
    "        a_max = np.argmax(Qvalues)      # choose the action with max Q value\n",
    "        a_agent = np.copy(a[a_max])     # return the index of action\n",
    "    return a_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1646428233854,
     "user": {
      "displayName": "段辉然",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04583225436557064813"
     },
     "user_tz": -60
    },
    "id": "kvYNYdd-vRuv",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodes: 0\n",
      "Episodes: 1000\n",
      "Episodes: 2000\n",
      "Episodes: 3000\n",
      "Episodes: 4000\n",
      "Episodes: 5000\n",
      "Episodes: 6000\n",
      "Episodes: 7000\n",
      "Episodes: 8000\n",
      "Episodes: 9000\n",
      "Episodes: 10000\n",
      "Episodes: 11000\n",
      "Episodes: 12000\n",
      "Episodes: 13000\n",
      "Episodes: 14000\n",
      "Episodes: 15000\n",
      "Episodes: 16000\n",
      "Episodes: 17000\n",
      "Episodes: 18000\n",
      "Episodes: 19000\n",
      "Episodes: 20000\n",
      "Episodes: 21000\n",
      "Episodes: 22000\n",
      "Episodes: 23000\n",
      "Episodes: 24000\n",
      "Episodes: 25000\n",
      "Episodes: 26000\n"
     ]
    }
   ],
   "source": [
    "# TRAINING LOOP\n",
    "\n",
    "# INIALIZE THE NUERAL NETWORK WE USE\n",
    "model=Neural_network()\n",
    "model.parameter_init()\n",
    "model.biases_init()\n",
    "\n",
    "# SAVING VARIABLES\n",
    "R_save = np.zeros([N_episodes, 1])           ## STORE THE REWARD PER GAME\n",
    "N_moves_save = np.zeros([N_episodes, 1])     ## STOR THE NUMBER OF MOVES PER GAME\n",
    "\n",
    "for n in range(N_episodes):\n",
    "    \n",
    "    if n % 1000 == 0:\n",
    "        print(\"Episodes:\", n)                ## print it every 1000 episodes\n",
    "\n",
    "    epsilon_f = epsilon_0 / (1 + beta * n)   ## DECAYING EPSILON\n",
    "    Done=0                                   ## SET DONE TO ZERO (BEGINNING OF THE EPISODE)\n",
    "    i = 1                                    ## COUNTER FOR NUMBER OF ACTIONS\n",
    "    \n",
    "    S,X,allowed_a=env.Initialise_game()      ## INITIALISE GAME (S: CHESSBOARD; X:STATE; ALLOWED_A: POSSIBLE ACTIONS)\n",
    "    \n",
    "    a,_=np.where(allowed_a==1)               ## THE POSSIBLE ACTIONS IN THE CURRENT STATE\n",
    "    a_agent = get_action(X,a)                ## CHOOSE A ACTION\n",
    "    \n",
    "    while Done==0:                           ## START THE EPISODE\n",
    "        \n",
    "\n",
    "        S_next,X_next,allowed_a_next,R,Done=env.OneStep(a_agent)  ## IMPLEMENT THE ACTION\n",
    "\n",
    "        ## THE EPISODE HAS ENDED, UPDATE\n",
    "        if Done==1:\n",
    "            \n",
    "            R_save[n]=np.copy(R)             ## STORE THE REWARD\n",
    "            N_moves_save[n]=np.copy(i)       ## STORE THE NUMBER OF MOVES\n",
    "            next_a_agent=0                   ## SET THIS TO 0 AS NO NEXT ACTION NEEDED\n",
    "            train_model(X,a_agent,R,X_next,next_a_agent,Done)  ## TRAIN MODEL AND UPDATE PARAMETER\n",
    "            break    \n",
    "        \n",
    "        # IF THE EPISODE IS NOT OVER...\n",
    "        else:\n",
    "            a_next,_=np.where(allowed_a_next==1)              ## THE POSSIBLE NEXT ACTIONS IN THE NEXT STATE\n",
    "            next_a_agent = get_action(X_next,a_next)          ## CHOOSE A ACTION FOR NEXT STATE\n",
    "            train_model(X,a_agent,R,X_next,next_a_agent,Done) ## TRAIN MODEL AND UPDATE PARAMETER\n",
    "            \n",
    "        # NEXT STATE AND CO. BECOME ACTUAL STATE...     \n",
    "        S=np.copy(S_next)                                     \n",
    "        X=np.copy(X_next)\n",
    "        allowed_a=np.copy(allowed_a_next)\n",
    "        a_agent=next_a_agent\n",
    "        \n",
    "        i += 1  # UPDATE COUNTER FOR NUMBER OF ACTIONS\n",
    "\n",
    "print('Deep_sarsa_Agent, Average reward:',np.mean(R_save),'Number of steps: ',np.mean(N_moves_save))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 821,
     "status": "ok",
     "timestamp": 1646427943735,
     "user": {
      "displayName": "段辉然",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04583225436557064813"
     },
     "user_tz": -60
    },
    "id": "uJ7mOJM2uhJc"
   },
   "outputs": [],
   "source": [
    "result_filename = f'result_hyperparameters_{gamma}_{beta}.xlsx' # Create a corresponding file name\n",
    "\n",
    "g=pd.DataFrame()\n",
    "g['R_save']=R_save.reshape(N_episodes)\n",
    "g['N_moves_save']=N_moves_save.reshape(N_episodes)\n",
    "g.to_excel(result_filename, encoding='utf-8',index=False) # store the results to excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 310,
     "status": "ok",
     "timestamp": 1646427952565,
     "user": {
      "displayName": "段辉然",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04583225436557064813"
     },
     "user_tz": -60
    },
    "id": "7XYygbweuhLo"
   },
   "outputs": [],
   "source": [
    "re=pd.read_excel(result_filename) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "executionInfo": {
     "elapsed": 441,
     "status": "ok",
     "timestamp": 1646427957713,
     "user": {
      "displayName": "段辉然",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04583225436557064813"
     },
     "user_tz": -60
    },
    "id": "qIUGK44yvBpg",
    "outputId": "312bd14f-7ab5-41d8-97b6-8969ca391e3e"
   },
   "outputs": [],
   "source": [
    "plt.plot(range(N_episodes), EMA(re['R_save']),c='blue',alpha=0.7,label=f'$\\gamma={gamma},\\\\beta={beta}$')\n",
    "plt.legend()\n",
    "plt.title('The reward per game vs training time(EMA)')\n",
    "plt.xlabel('training time')\n",
    "plt.ylabel('The reward per game')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "executionInfo": {
     "elapsed": 472,
     "status": "ok",
     "timestamp": 1646427961335,
     "user": {
      "displayName": "段辉然",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04583225436557064813"
     },
     "user_tz": -60
    },
    "id": "EB5qPrf4vBxA",
    "outputId": "918e4dcc-47ff-41d6-9f58-aa7ebfe0f631"
   },
   "outputs": [],
   "source": [
    "plt.plot(range(N_episodes), EMA(re['N_moves_save']),c='b',alpha=0.7,label=f'$\\gamma={gamma},\\\\beta={beta}$')\n",
    "plt.legend()\n",
    "plt.title('The number of moves per game vs training time(EMA)')\n",
    "plt.xlabel('training time')\n",
    "plt.ylabel('The number of moves per game')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VGmHgzkGvBzT"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a5Ckam4vvB1h"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AlstSF5hvB52"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "06XM11alvCAw"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMXDf812ZmQV9bBiAGjKvBr",
   "collapsed_sections": [],
   "mount_file_id": "1q0g24jjI8gfpeT26o73IyC5p9OsdhZIB",
   "name": "Hyper_Change.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
